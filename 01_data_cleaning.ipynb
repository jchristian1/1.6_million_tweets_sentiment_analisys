{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc7d2de-4c9c-450e-a798-ddc7c89355d9",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis: Data Cleaning\n",
    "\n",
    "## Notebook Overview\n",
    "This notebook is dedicated to cleaning and preparing the Twitter dataset for subsequent sentiment analysis. The primary goal is to transform raw tweet data into a clean format suitable for NLP tasks. This involves handling missing data, normalizing text, and removing noise such as special characters and URLs.\n",
    "\n",
    "## Table of Contents\n",
    "1. **Introduction**\n",
    "   - Purpose of the notebook\n",
    "   - Description of the dataset\n",
    "2. **Data Loading**\n",
    "   - Import necessary libraries\n",
    "   - Load the dataset\n",
    "3. **Initial Data Exploration**\n",
    "   - Display basic information and statistics\n",
    "   - Identify missing values and anomalies\n",
    "4. **Data Cleaning**\n",
    "   - Remove unnecessary columns\n",
    "   - Normalize the target Column\n",
    "   - Handle missing data\n",
    "   - Normalize text data\n",
    "   - Remove special characters, URLs, mentions, and hashtags\n",
    "6. **Saving the Cleaned Data**\n",
    "   - Save the processed data to a new CSV file\n",
    "7. **Conclusion**\n",
    "   - Summary of the data cleaning steps\n",
    "   - Next steps and transition to the next phase of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadd7ea-c2ea-4276-8851-f8d4a291b229",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "The focus of this notebook is to clean the data extracted from Twitter, ensuring it is primed for analysis and modeling. The dataset contains various columns from which only the text and sentiment labels are primarily needed for our sentiment analysis task.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "The dataset you are using is the \"Sentiment140\" dataset. This dataset was created by researchers at Stanford University and is designed specifically for sentiment analysis tasks. It is commonly used in the machine learning community for training and testing sentiment analysis models due to its large volume and diversity of tweets. This dataset can be found here: https://www.kaggle.com/datasets/milobele/sentiment140-dataset-1600000-tweets\n",
    "\n",
    "## Dataset Composition\n",
    "\n",
    "The Sentiment140 dataset consists of 1.6 million tweets extracted using the Twitter API. The tweets have been annotated automatically with tags related to the sentiment of the tweet based on the emoticons present in them. Here are the details of the columns in the dataset:\n",
    "\n",
    "target: The polarity of the tweet (0 = negative, 2 = neutral, 4 = positive). In most versions of this dataset, only negative and positive sentiments are included (0 and 4).\n",
    "\n",
    "##### id: The unique identifier for each tweet.\n",
    "##### date: The date and time when the tweet was posted.\n",
    "##### flag: This field is often set to \"NO_QUERY\" and does not contain useful information for sentiment analysis.\n",
    "##### user: The username of the tweet's author.\n",
    "##### text: The text of the tweet itself.\n",
    "\n",
    "This structure helps in various NLP tasks, particularly sentiment analysis, by providing a pre-labeled set of data for training machine learning models.\n",
    "\n",
    "When loading this data, ensure you handle it correctly by specifying the structure and encoding, as shown previously. This will set a solid foundation for your data cleaning and analysis processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17493834-e28c-4176-8acd-5e7e0e27399d",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "376445ab-e133-433e-b024-88fcadede540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743d80d2-ec2e-49b9-8b3e-b268d5b9be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names for the dataset\n",
    "file_path = 'raw_data/training.1600000.processed.noemoticon.csv'\n",
    "column_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, names=column_names, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d00ad-730f-4c87-9adf-dc6611b7ee78",
   "metadata": {},
   "source": [
    "#### Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61206d73-2c35-4c2e-8958-0e31e32417b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3eb1b9-fe5d-4545-86e4-885424f4f286",
   "metadata": {},
   "source": [
    "# 3. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f6e4f-80c9-418f-8195-fe0a39008c4a",
   "metadata": {},
   "source": [
    "#### Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5b443a4-0009-44bb-a35d-177b5aa8361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   id      1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61335a2a-90fc-442a-b5c7-e25dec821001",
   "metadata": {},
   "source": [
    "#### Let's parse the date column to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c565427d-7ac4-4223-b691-7c3f0ff1a44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Mon Apr 06 22:19:45 PDT 2009\n",
       "1    Mon Apr 06 22:19:49 PDT 2009\n",
       "2    Mon Apr 06 22:19:53 PDT 2009\n",
       "3    Mon Apr 06 22:19:57 PDT 2009\n",
       "4    Mon Apr 06 22:19:57 PDT 2009\n",
       "5    Mon Apr 06 22:20:00 PDT 2009\n",
       "6    Mon Apr 06 22:20:03 PDT 2009\n",
       "7    Mon Apr 06 22:20:03 PDT 2009\n",
       "8    Mon Apr 06 22:20:05 PDT 2009\n",
       "9    Mon Apr 06 22:20:09 PDT 2009\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "336a139f-cec2-4832-bc91-6581241816b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timezone Counts:\n",
      "timezone\n",
      "PDT    1600000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Correct timezone extraction\n",
    "data['timezone'] = data['date'].str.extract(r\"\\b([A-Z]{3})\\b\")\n",
    "\n",
    "# Display timezone counts\n",
    "timezone_counts = data['timezone'].value_counts(dropna=False)  # Include NaN counts\n",
    "print(\"Timezone Counts:\")\n",
    "print(timezone_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "642ce491-0a07-4c1c-990f-945daa66c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of timezone abbreviations to UTC offsets\n",
    "timezone_map = {\n",
    "    \"PDT\": \"-0700\",\n",
    "    \"EDT\": \"-0400\",\n",
    "    \"CST\": \"-0600\",\n",
    "    \"MST\": \"-0700\",\n",
    "    \"EST\": \"-0500\",\n",
    "}\n",
    "\n",
    "# Replace timezone abbreviations with offsets in the date column\n",
    "for tz, offset in timezone_map.items():\n",
    "    data['date'] = data['date'].str.replace(tz, offset, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2b16bfb-20a4-400a-adcf-50c8fdcd7992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-04-06 22:19:45-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-04-06 22:19:49-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-04-06 22:19:53-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-04-06 22:19:57-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-04-06 22:19:57-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date\n",
       "0 2009-04-06 22:19:45-07:00\n",
       "1 2009-04-06 22:19:49-07:00\n",
       "2 2009-04-06 22:19:53-07:00\n",
       "3 2009-04-06 22:19:57-07:00\n",
       "4 2009-04-06 22:19:57-07:00"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the date column with timezone offsets\n",
    "date_format_with_offset = \"%a %b %d %H:%M:%S %z %Y\"  # %z handles offsets\n",
    "data['date'] = pd.to_datetime(data['date'], format=date_format_with_offset, errors='coerce')\n",
    "\n",
    "# Verify the parsing\n",
    "data[['date']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee3998-ee4d-474b-979c-31e5cae088e8",
   "metadata": {},
   "source": [
    "##### Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52455bcb-8d89-402d-8d83-2595004997b7",
   "metadata": {},
   "source": [
    "#### Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e42d37d-5519-43db-a1a1-5eff894ac836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935761e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target            id\n",
       "count  1.600000e+06  1.600000e+06\n",
       "mean   2.000000e+00  1.998818e+09\n",
       "std    2.000001e+00  1.935761e+08\n",
       "min    0.000000e+00  1.467810e+09\n",
       "25%    0.000000e+00  1.956916e+09\n",
       "50%    2.000000e+00  2.002102e+09\n",
       "75%    4.000000e+00  2.177059e+09\n",
       "max    4.000000e+00  2.329206e+09"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773930e6-3eb7-4065-a702-68a4e7408d53",
   "metadata": {},
   "source": [
    "#### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b592bfe-8b4e-46e8-872c-d5a67c074e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                          int64\n",
       "id                              int64\n",
       "date        datetime64[ns, UTC-07:00]\n",
       "flag                           object\n",
       "user                           object\n",
       "text                           object\n",
       "timezone                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6aabf3-4bf1-493d-88e8-7b9f0f1f535d",
   "metadata": {},
   "source": [
    "#### Let's identify missing values and anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d36a5332-b139-4289-a7c1-a595923e8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Count:\n",
      "target      0\n",
      "id          0\n",
      "date        0\n",
      "flag        0\n",
      "user        0\n",
      "text        0\n",
      "timezone    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "print(\"Missing Values Count:\")\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bc8dd27-e1d2-4628-9b72-6c1ef875d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'target':\n",
      "[0 4]\n",
      "Are IDs unique? False\n"
     ]
    }
   ],
   "source": [
    "# Unique values in the target column\n",
    "print(\"Unique values in 'target':\")\n",
    "print(data['target'].unique())\n",
    "\n",
    "# Check for unique IDs\n",
    "is_id_unique = data['id'].is_unique\n",
    "print(f\"Are IDs unique? {is_id_unique}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d03dae29-35c6-4685-bbe1-7fa358c68bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1581466 unique texts.\n",
      "This means we need to drop 18534 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "# Unique values in the 'text' column\n",
    "unique_texts_count = len(data['text'].unique())  # Count of unique texts\n",
    "total_rows = len(data)  # Total number of rows in the dataset\n",
    "\n",
    "# Calculate duplicates\n",
    "duplicates_count = total_rows - unique_texts_count\n",
    "\n",
    "# Print results\n",
    "print(f\"There are {unique_texts_count} unique texts.\")\n",
    "print(f\"This means we need to drop {duplicates_count} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127d492-3ef2-4b30-ba91-eacd8568f1ff",
   "metadata": {},
   "source": [
    "Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b7fd21d-755a-47fd-9abe-dcb4f822fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after dropping duplicates: 1581466\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates based on the 'text' column\n",
    "data.drop_duplicates(subset=[\"text\"], inplace=True)\n",
    "\n",
    "# Reset the index for the updated DataFrame\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Confirm the new size of the dataset\n",
    "print(f\"Dataset size after dropping duplicates: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "789dc760-f785-4bef-9c04-c723bbc2cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of very short text entries: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for empty or very short text\n",
    "short_texts = data[data['text'].str.len() < 5]\n",
    "print(f\"Number of very short text entries: {len(short_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb9724-2991-4b69-bc84-aeb0ba34de77",
   "metadata": {},
   "source": [
    "Let's drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c70fdcad-0a16-40d0-8655-362ad79a668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of very short text entries: 0\n",
      "Dataset size after dropping very short text entries: 1581466\n"
     ]
    }
   ],
   "source": [
    "# Identify and count very short text entries\n",
    "short_texts = data[data['text'].str.len() < 5]\n",
    "print(f\"Number of very short text entries: {len(short_texts)}\")\n",
    "\n",
    "# Drop short text entries\n",
    "data = data[data['text'].str.len() >= 5].reset_index(drop=True)\n",
    "\n",
    "# Confirm the new size of the dataset\n",
    "print(f\"Dataset size after dropping very short text entries: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8290e-0ee0-41e2-8429-3c674c763b3d",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb3e14-1ef9-4ad1-9742-5ae7837ed840",
   "metadata": {},
   "source": [
    "#### Let's drop irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a95a724d-70c7-4c22-bbc1-84fe4e5b2123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:45-07:00</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:49-07:00</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:53-07:00</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57-07:00</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57-07:00</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                      date  \\\n",
       "0       0 2009-04-06 22:19:45-07:00   \n",
       "1       0 2009-04-06 22:19:49-07:00   \n",
       "2       0 2009-04-06 22:19:53-07:00   \n",
       "3       0 2009-04-06 22:19:57-07:00   \n",
       "4       0 2009-04-06 22:19:57-07:00   \n",
       "\n",
       "                                                text  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop irrelevant columns\n",
    "data = data.drop(columns=['id', 'flag', 'user', 'timezone'])\n",
    "\n",
    "# Verify the updated DataFrame structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef435651-e5c5-4d47-925b-0f6d4188929f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'date', 'text'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8e641-7f51-4869-9b4f-ff4ec7d3b19c",
   "metadata": {},
   "source": [
    "#### Normalize the target Column (0 will be negative and 1 will be positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "619b6acd-c1b0-4a3a-a133-07c8f09022c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'target' after normalization:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the target column\n",
    "data['target'] = data['target'].replace({4: 1})\n",
    "\n",
    "# Verify normalization\n",
    "print(\"Unique values in 'target' after normalization:\")\n",
    "print(data['target'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d02b7f-ec92-449e-9949-ac8fb8c8be07",
   "metadata": {},
   "source": [
    "#### Normalize text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5238afc2-6107-4d4e-9662-a8c7a2c6fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample normalized text:\n",
      "0    a thats a bummer you shoulda got david carr of...\n",
      "1    is upset that he cant update his facebook by t...\n",
      "2    i dived many times for the ball managed to sav...\n",
      "3       my whole body feels itchy and like its on fire\n",
      "4    no its not behaving at all im mad why am i her...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define a function to normalize text\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')  # Normalize encoding\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+|ftp\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply normalization to the text column\n",
    "data['text'] = data['text'].apply(normalize_text)\n",
    "\n",
    "# Verify the results\n",
    "print(\"Sample normalized text:\")\n",
    "print(data['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffae19-a6bd-44aa-93a9-27f2ebf9a21c",
   "metadata": {},
   "source": [
    "#### Now let's validate the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c0313b-d1ad-4b60-b482-fbeac5324be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "{'mentions': 0, 'urls': 17, 'hashtags': 0, 'special_characters': 0}\n"
     ]
    }
   ],
   "source": [
    "def validate_cleaning(data):\n",
    "    \"\"\"\n",
    "    Validates if all mentions, URLs, hashtags, and special characters are removed.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The DataFrame containing the `text` column.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Counts of remaining unwanted elements.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        \"mentions\": r\"@\\w+\",\n",
    "        \"urls\": r\"http\\S+|www\\S+|https\\S+\",\n",
    "        \"hashtags\": r\"#\\w+\",\n",
    "        \"special_characters\": r\"[^\\w\\s]\",\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for key, pattern in patterns.items():\n",
    "        matches = data['text'].str.contains(pattern, regex=True).sum()\n",
    "        results[key] = matches\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the validation\n",
    "validation_results = validate_cleaning(data)\n",
    "\n",
    "# Display the results\n",
    "print(\"Validation Results:\")\n",
    "print(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0946cf-e8fc-4e0d-9879-dae86f5215f8",
   "metadata": {},
   "source": [
    "#### Display some random samples of the text column to double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e72bca-ebf9-4976-b526-4db9a17fb7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Samples of Text:\n",
      "1: hans im an open book you can ask me anything you want hit up the blog and ask away i can give you more in depth answer there\n",
      "2: wuaaah we have strawberries at home\n",
      "3: very cool and to quote tbs quotnow thats funnyquot\n",
      "4: the last sunday\n",
      "5: oh thanks thats good to know\n",
      "6: hi kate how are you\n",
      "7: nice i like that combo\n",
      "8: youre always welcome just give me a headsup so i can put down the machete and rinse off the bug spray\n",
      "9: excited for tomorrows swim date\n",
      "10: i want to call you but i live in braaaaazil xoxo\n",
      "11: need a wireless card or usb hook up for my computer theres no cable jack in my room amp nor can they put one in\n",
      "12: is it possible to have enough beer haha jk they started running out when i went but i was dd so i barely sipped them\n",
      "13: yeah but now you wont be sitting with us and austins having trouble getting tickets\n",
      "14: hoping megan is having a good birthday and wondering where everyone is\n",
      "15: gave up with the studyin thing watchhinn tv\n",
      "16: great loss indeed i hope the find what went wrong and prevent future incidents\n",
      "17: now thats celebrating in style\n",
      "18: came out the club now lookin for somewhere to eat mofongo house is too packed\n",
      "19: sorry about your knee dear now wheres that tattoo pic\n",
      "20: new blackberry baby woop shopping today was fun fun fun\n",
      "21: joe wants to have sex but yvonne ir a good girl hahaha\n",
      "22: getting on the typhoon last ride of the night\n",
      "23: jonas brothers omjjj i love yous both should have been a tie woo lets fuck haha jkjk\n",
      "24: same for me except i dont have any internet\n",
      "25: gotta run radio interview in a few minutes getting my thoughts in order\n",
      "26: my pic with harry is so bad\n",
      "27: yes it was horrible dont wanna go back ever such a dark place\n",
      "28: what is her favorite color\n",
      "29: to the corner store or to one in another country im looking forward to when i can go to beijing to have a drink\n",
      "30: well they dont know what theyre missing ive ordered from glamourdolleyes amp sudnsass cause of your vids\n",
      "31: sigh got the disconnect notice for the power this am all i need now is to find a thousand dollars\n",
      "32: i hate our phones\n",
      "33: what does one wall say to the other meet you in the corner incredible lame watching the fashion show on youtube lt3\n",
      "34: i want my cd\n",
      "35: i told you big head\n",
      "36: today dragged on but atleast im home nao might go out inaa bit\n",
      "37: morning everyone how are yall doing\n",
      "38: elvas babyshower todayyy with bran da man its a boy stokkkedd tonight should be good hollerzz\n",
      "39: awww cuuute i have a black cat too but shes only 2 months old\n",
      "40: glad i could help anytime\n",
      "41: gosh this cough is killing me\n",
      "42: cant wait because she has her prom very soon\n",
      "43: camping was awesome we rented a pontoon and we went tubing and we made smores and told ghost stories so much fun but now i have a cold\n",
      "44: what kind of issues and what kind of plane im a aircraft technician cant help you just curious\n",
      "45: calling a seat for nazi nick in nw\n",
      "46: thanx 4 following hope ur doing well\n",
      "47: hugs glad to hear that hope youre lil doll is ok s\n",
      "48: a i miss my friends already oh well hopefully i will get the chance to see the majority of them over the summer stomach hurts\n",
      "49: barsexuals is on the scoop againimpressive\n",
      "50: sitting here stuck work wanna watch the game shawn\n",
      "51: i like them\n",
      "52: agh phones keep ringing stirring up another migrain hope it goes away before my dr appointment later\n",
      "53: at work and so upset i cant get my phone yet to update twitter\n",
      "54: my conturytomorrow is monday\n",
      "55: seven days until i see my wife again\n",
      "56: hmmm hehe i wonder where my laxatives are\n",
      "57: i love them too did you signup on shoehunting you can save shoes and it will get posted to twitter automatically fun\n",
      "58: actual last day of college pity it has to finish with a drama examtotal crap\n",
      "59: always a great thing to find in your inbox\n",
      "60: happy cause boyfriend will come over and tuck me in tonight\n",
      "61: im being misserable lol why am i not good enough to be someones girlfriend it sucks to be me\n",
      "62: missing a very special friend right now\n",
      "63: lol i recommend you a lot i have no idea who it might be amber maybe\n",
      "64: i am feeling so ill today my throat is swollen and i just feel ugh bad times\n",
      "65: so i got a new battery and that was half the problem now the connectors need to be replaced so i am still without my car\n",
      "66: \n",
      "67: your pic doesnt show up and it makes me sad\n",
      "68: now act like it\n",
      "69: thanks for the set list do you know if theres any pixie merch x\n",
      "70: lol that one came through blank\n",
      "71: nope u have just got a susan boyle thang goin on haha\n",
      "72: my back is killing me\n",
      "73: so sleepy today its too hot and nice to be in work all day\n",
      "74: wiiii i hear radio disney again\n",
      "75: getting ready to start work it is to nice a day to work in side\n",
      "76: all icetea is you from x\n",
      "77: woke up with the most massive hairdoo proper all over the place jungle afro its raining and im cold\n",
      "78: your tweet was just included in the longest poem in the world\n",
      "79: thats cool\n",
      "80: having strep throat stinks\n",
      "81: oh god i need this phone amp the n97 touchscreen and keypad the optical keypad will make it such a twitter client\n",
      "82: hi sarah yes great movie so many tears haha\n",
      "83: you off today if so enjoy\n",
      "84: wishing i was outside frolicking instead of folding past due invoices\n",
      "85: prom is scary hahaha\n",
      "86: sleep would be marvelous right now my feets hurt alas i need a shower first sigh\n",
      "87: hey i was gonna start one caught me\n",
      "88: chilling at home\n",
      "89: ciara was great at snlkimyou were mentioned on snl tonightjustin t was really good also\n",
      "90: i knew it wouldnt last it was only a matter of time sux i was getting used to his nice side\n",
      "91: for all those ex gis soon to be ex manpower people i wish you well in the job hunt dont you just love offshoring\n",
      "92: m lemon yes we have no flv support in quicktime x for snow leopard developer preview bad source\n",
      "93: im a production assistant which is the bottom of the ladder in the production worldbut its on the ladder at least\n",
      "94: want to eat kinder bueno white now\n",
      "95: i knew i had sun stroke just got sent home sick\n",
      "96: omg why wont my mom hurry up ltxoxogt\n",
      "97: thanks for the tip this author has great songs\n",
      "98: uh doughnuts im not eating todayi feel disgusting\n",
      "99: according to mick jagger hindu ragas are more sfs sort of music\n",
      "100: i gotta sleep later tweeties i really had a fun time its already 310am here thanks a lot to yall\n",
      "101: has tonsils that are so swollen they are touching each other\n",
      "102: harvested another rogue lettuce in my carrots which was attracting the slugs urgh\n",
      "103: going 2 the ihop yum yum\n",
      "104: loves sundays so sore this morning but off to the gym anyways\n",
      "105: howdy maam keeping zona entertained\n",
      "106: affects every tiny little bit of our personal lives dho\n",
      "107: mrs gritmaster has just signed up to twitter not sure she knows what she is doing anyway welcome to twitter\n",
      "108: umm i think six or seven if you can think of any i would love to hear them\n",
      "109: a not on\n",
      "110: btw was dave there too cuz doesnt he like sports a lot lol\n",
      "111: it was a good day learned how i can be better at my job got my cardio in and got to watch the lost finale with andy\n",
      "112: volcanic erruption in my noggin aka massive headache\n",
      "113: thanks\n",
      "114: you may want to keep a stash of those pills just in case\n",
      "115: im in a car driving to st pete for a funeral i have a bb but no cable tv\n",
      "116: unfortunately the blackampred lost but i dont mind im from pernambuco and sport is my alternative\n",
      "117: the vt storm chasers are on twitter follow them at i wish i could be with them however i dont miss all the driving\n",
      "118: in pittsburgh finally going to see sissylt3 and unfortunatly her bitch girlfriend err\n",
      "119: dont give up on life\n",
      "120: wow booking flights is really hard work how do i know which is cheapest\n",
      "121: watchingkim possible lol\n",
      "122: had a great meeting decided on our new club name too its boogie bam anime jam let that sink in\n",
      "123: nooo apples server is temporarily unavailable as i was finishing the 30 download thats what i get for waiting for dramatic effect\n",
      "124: hmm wondering if im happy having my firefox skin be christmasy all the time may need some getting used to\n",
      "125: whats happened x\n",
      "126: watched 4 eps of liar game and am loving it will watch the rest over the next week or two and get the blog done asap\n",
      "127: my baby niece is graduating from kindergarden today\n",
      "128: getting my drink on since its my birthday hell yeah\n",
      "129: yeah the 1k15k ill be saving should buy a lotta drink for me\n",
      "130: hey parishow r u what r u going to buy today i would like to go shopping with u someday xoxoxo\n",
      "131: noo flawless shud have won\n",
      "132: pretty sure east17 are someone we would all love to see reform it was cool for boys to like boybands back then\n",
      "133: okay thanks e\n",
      "134: my best buds in the whole world claudias not there though she is gone to cuba miss her\n",
      "135: cant working\n",
      "136: will go to wedding with my mum say quot grow upquot but she cant understand dont wanna go\n",
      "137: hey awesome ill go check it out dude\n",
      "138: how about visiting canada preferrably toronto ontario area\n",
      "139: your offended by my use of the word quotthequot okmagazine i cant believe i have offended you\n",
      "140: this isnt going very well why is it taking so long\n",
      "141: jay def brings an energy when he steps in but thats to be expected hes 19 amazing hes only been playing 6 years or so\n",
      "142: when are you coming to philly hurry please\n",
      "143: should have never gone home for the weekendtalk about being homesick\n",
      "144: we met elijahs girl friend she is really nice but she is a quotnormalquot horse person so i have to teach her parelli but she loved blaze\n",
      "145: yo do you have gordons actual interview questions as i have the hnd ones\n",
      "146: i got the cd in the mail\n",
      "147: ohhh yahh o i member chuu x3 hiii\n",
      "148: one of the vehicles on service 13 has suffered a mechanical problem at bathford engineers on way apologies for any delays\n",
      "149: so he should it beautiful\n",
      "150: quotim breaking windquot lmao after school care kids are funny\n",
      "151: very true\n",
      "152: ohhh what do to what to do what to do\n",
      "153: of course we were at 145am jordens heart got broke yesterday\n",
      "154: out spoiling myself im really happy\n",
      "155: outpost gallifrey and its forum will be closing on july 31 wtf hope theres going to be a good new alternative\n",
      "156: put up the brenna is ghey video and the ones from my party last year\n",
      "157: i needa stop comparing myself to u\n",
      "158: sorry wrong person boss ladu is has another name yet to be determined\n",
      "159: thanks henrik allow me 9 days for delivery so sorry but new stock is coming next tuesday not earlier\n",
      "160: me either\n",
      "161: its like she doesnt want to come\n",
      "162: i hope that comment jinxed your lakers\n",
      "163: totally narked with my recent barber experience far too short for comfort he was bald maybe he hated my flowing locks\n",
      "164: yeah that would be dumb buddy lol\n",
      "165: oh yeah im the original wild child with my dew drinking\n",
      "166: is wondering when shell be able to see cherry bomb\n",
      "167: yea green\n",
      "168: fun i had to try that one\n",
      "169: really wish i could be at the laker parade tomorrow to celebrate with my bestfriendslakerfans\n",
      "170: i have a rash on my chest\n",
      "171: happy mothers day to your mom and wife today\n",
      "172: tom the show in brasilia was amazing was my birthday and i love you sow much why arent you talking to me\n",
      "173: beautiful art garfunkel is a true minstrel and pauls voice moves me endlessly\n",
      "174: its beautiful there i would know i went there just to see a concert\n",
      "175: is uploading his new profilephoto i think ita s ok\n",
      "176: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaah\n",
      "177: trying to get out of bed\n",
      "178: sounds like fun and hope you have a wonderful time but dont get too crazy ya hear\n",
      "179: ou gush i didint wrote si long bur what can i say maybe i am busy or what but keri hilson is always in my mind these days\n",
      "180: not only was your talk impressive the fact you remained standing throughout it was astounding well done sir neutral\n",
      "181: 15pm oh must be a wonderful session for u and chris\n",
      "182: where r all the hot fla ladies seems all the sex kittens i am following r in cali\n",
      "183: is not going to let others bring me down life goes on\n",
      "184: thanks friendim jelly ur having roscoes right now\n",
      "185: azn grocery shopppping\n",
      "186: wants to drive around redlands at sunset put on some good music and feel the cool breeze on his face while the palm trees sway\n",
      "187: woah time goes past really quickly when im writing a text its kinda annoying missing out on information\n",
      "188: opera is not working thank god i have safari\n",
      "189: is pale sick and too tired for work\n",
      "190: but stuck at home coz of the rains and a sore throat\n",
      "191: is going to dinner wo mom\n",
      "192: getting up with the kids never a break buy i got to love their smiles\n",
      "193: was obviously very wrong when i thought my feet were slowly getting better x\n",
      "194: jb had and awsome ride\n",
      "195: hope you didnt have trouble translatin mine haha\n",
      "196: i bought speed racer today at hollywood videofor 4 bucks i am gonna go watch it and cool off from all the walking i did today\n",
      "197: panicking slightly running out of healthy foods to suppliment my lack of suppliments s next pay day mens protien amp lots of it\n",
      "198: i love that invite its so cute you better design your own wedding invite i bet youll have a really nice story to share\n",
      "199: cleaned the kitchen whilst drinking beer and listening to green day full blast was about to have a bath but the monster got there first\n",
      "200: kfc is so addictive fighting the urge 2 jump in my car my sisters tweet is egging me on\n"
     ]
    }
   ],
   "source": [
    "random_samples = data['text'].sample(200, random_state=42)\n",
    "\n",
    "print(\"Random Samples of Text:\")\n",
    "for i, text in enumerate(random_samples, start=1):\n",
    "    print(f\"{i}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195816e-d758-48cd-ae6d-731e1f5ef0eb",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "#### Weirdly Encoded Characters\n",
    "- **Example**: `fckbri12ndby 40` (Sample 49)  \n",
    "  - This still looks unusual and might indicate an encoding issue or nonsensical text entry.\n",
    "  - Likely originated from corrupted data or abbreviation/slang.\n",
    "\n",
    "#### Short/Unclear Contexts\n",
    "- **Examples**:  \n",
    "  - `high low` (Sample 165)  \n",
    "  - `diversity won damnn i wanted flawless to win` (Sample 160)  \n",
    "  - `downtown disney` (Sample 200)  \n",
    "  - These entries are brief and may lack meaningful context for sentiment analysis.\n",
    "\n",
    "#### Edge Cases\n",
    "- **Example**: `morning twitterlandoff to work i go` (Sample 176)  \n",
    "  - Contains typos or casual language (`twitterland`), which is common in tweets but may affect NLP preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3bacd-7653-4257-b068-a0d632de1707",
   "metadata": {},
   "source": [
    "# 5. Saving the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f11afc3-10ca-4455-8816-5305580ea642",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./clean_data/cleaned_twitter_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266dbdd-f3d3-426c-94b3-c8ab8d9817e6",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24167077-10e8-48fa-9217-7a5b3b9a4773",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we successfully cleaned the raw text data from the Sentiment140 dataset. Below is a summary of the tasks completed:\n",
    "\n",
    "1. **Loaded the Dataset**:\n",
    "   - Imported the Sentiment140 dataset and examined its structure and data types.\n",
    "\n",
    "2. **Handled Missing Values and Anomalies**:\n",
    "   - Verified that no missing values were present.\n",
    "   - Ensured no duplicate rows exist.\n",
    "   - Identified and addressed encoding issues and nonsensical entries.\n",
    "\n",
    "3. **Formatted the `date` Column**:\n",
    "   - Standardized the `date` column by replacing timezone abbreviations with UTC offsets.\n",
    "   - Converted the column to the correct `datetime` datatype for consistency.\n",
    "\n",
    "4. **Normalized Text Data**:\n",
    "   - Removed mentions, hashtags, URLs, and special characters.\n",
    "   - Converted text to lowercase for uniformity.\n",
    "   - Validated the cleaning process to ensure all unwanted elements were removed.\n",
    "\n",
    "\n",
    "\n",
    "With this, the data is now clean and ready for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Exploratory Data Analysis (EDA)\n",
    "\n",
    "The next notebook will focus on exploring the cleaned dataset. This will include:\n",
    "- Visualizing the distribution of sentiment classes.\n",
    "- Analyzing tweet characteristics (e.g., text length, word usage).\n",
    "- Identifying patterns and relationships that may influence sentiment.\n",
    "\n",
    "This EDA will provide valuable insights to guide the preprocessing and modeling phases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
