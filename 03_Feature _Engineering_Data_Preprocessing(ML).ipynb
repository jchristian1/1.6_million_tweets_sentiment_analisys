{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1077fd95-8e07-4fac-b217-c7fa160342c1",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis: Feature Engineering and Data Preprocessing\n",
    "\n",
    "## Notebook Overview\n",
    "This notebook is dedicated to preparing the cleaned Sentiment140 dataset for machine learning. The goal is to transform the raw text data into a format suitable for modeling by applying feature engineering techniques and preprocessing steps. This includes text normalization, stopword removal, expansion of contractions, and tokenization. We will also generate the necessary features from the temporal data and n-grams to optimize our sentiment analysis model.\n",
    "\n",
    "## Table of Contents\n",
    "1. **Introduction**  \n",
    "   - Purpose of the notebook  \n",
    "   - Description of the cleaned dataset  \n",
    "\n",
    "2. **Text Preprocessing**  \n",
    "   - Removal of stopwords  \n",
    "   - Expansion of contractions  \n",
    "   - Lemmatization and Tokenization  \n",
    "   - Text vectorization (TF-IDF and CountVectorizer)  \n",
    "\n",
    "3. **Feature Engineering**  \n",
    "   - Temporal features (Day of the week, Hour of the day, Month)  \n",
    "   - N-gram feature extraction (Unigrams, Bigrams, Trigrams)  \n",
    "\n",
    "4. **Handling Imbalanced Data**  \n",
    "   - Techniques for balancing sentiment classes (SMOTE, undersampling)  \n",
    "\n",
    "5. **Train/Test Split**  \n",
    "   - Splitting the dataset into training and testing sets  \n",
    "\n",
    "6. **Conclusion**  \n",
    "   - Summary of feature engineering and preprocessing steps  \n",
    "   - Next steps and preparation for modeling\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2c75c-d610-40fa-a104-73e3566dc30c",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "### Purpose of the Notebook\n",
    "The purpose of this notebook is to perform feature engineering and preprocessing on the cleaned Sentiment140 dataset to prepare it for sentiment analysis modeling. This will involve transforming the text data into numerical features that can be used by machine learning algorithms, while also addressing common challenges like imbalanced classes.\n",
    "\n",
    "### Description of the Cleaned Dataset\n",
    "The dataset under analysis is the cleaned version of the Sentiment140 dataset. After performing initial data cleaning in previous steps, the dataset consists of:\n",
    "\n",
    "- **Sentiment Labels**:  \n",
    "  - 0: Negative sentiment  \n",
    "  - 1: Positive sentiment\n",
    "\n",
    "- **Text Column**:  \n",
    "  Contains the normalized text of tweets, ready for analysis. The text has been cleaned of URLs, mentions, and special characters.\n",
    "\n",
    "- **Date Column**:  \n",
    "  The date column has been standardized to the correct datetime format with UTC offsets, allowing for temporal analysis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fa3fc7-089d-433f-8993-e3af9ad75972",
   "metadata": {},
   "source": [
    "#### Let's import the necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6422a7-1ea0-4c6a-8af8-305a6c09ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "# run this once only\n",
    "#nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4795d623-a016-4073-ade7-f963ff5a08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = './clean_data/cleaned_twitter_data_After_EDA.csv'\n",
    "data = pd.read_csv(file_path, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6d3a56-8c25-4bd9-9cd1-fa7cca677c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:45-07:00</td>\n",
       "      <td>a thats a bummer you shoulda got david carr of...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:49-07:00</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:53-07:00</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57-07:00</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-06 22:19:57-07:00</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                       date  \\\n",
       "0       0  2009-04-06 22:19:45-07:00   \n",
       "1       0  2009-04-06 22:19:49-07:00   \n",
       "2       0  2009-04-06 22:19:53-07:00   \n",
       "3       0  2009-04-06 22:19:57-07:00   \n",
       "4       0  2009-04-06 22:19:57-07:00   \n",
       "\n",
       "                                                text  text_length  day_of_week  \n",
       "0  a thats a bummer you shoulda got david carr of...         67.0            0  \n",
       "1  is upset that he cant update his facebook by t...        104.0            0  \n",
       "2  i dived many times for the ball managed to sav...         76.0            0  \n",
       "3     my whole body feels itchy and like its on fire         46.0            0  \n",
       "4  no its not behaving at all im mad why am i her...         85.0            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b23406a-ce7f-4e42-8579-9b1b181a6691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target           int64\n",
       "date            object\n",
       "text            object\n",
       "text_length    float64\n",
       "day_of_week      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400e4d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578237, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2280d7c-1881-4edc-849f-f4f55be42fd3",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing\n",
    "\n",
    "In this section, we will apply several preprocessing techniques to the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd1e7b-92f6-472f-a5b6-654566279d24",
   "metadata": {},
   "source": [
    "**Stopword Removal**:  \n",
    "  Removing common words that don't add much value to sentiment analysis (e.g., \"the\", \"and\", \"is\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69a1763-4bdd-453e-bf98-4397aa5f7ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text after stopword removal:\n",
      "                                                text  \\\n",
      "0  a thats a bummer you shoulda got david carr of...   \n",
      "1  is upset that he cant update his facebook by t...   \n",
      "2  i dived many times for the ball managed to sav...   \n",
      "3     my whole body feels itchy and like its on fire   \n",
      "4  no its not behaving at all im mad why am i her...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0      thats bummer shoulda got david carr third day  \n",
      "1  upset cant update facebook texting might cry r...  \n",
      "2  dived many times ball managed save 50 rest go ...  \n",
      "3                   whole body feels itchy like fire  \n",
      "4                           behaving im mad cant see  \n"
     ]
    }
   ],
   "source": [
    "# Define the function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())  # Convert to lowercase for uniformity\n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove stopwords from the text\n",
    "    filtered_text = [word for word in words if word not in stop_words]\n",
    "    # Rebuild the text\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Apply stopword removal to the 'text' column\n",
    "data['cleaned_text'] = data['text'].apply(remove_stopwords)\n",
    "\n",
    "# Verify the results\n",
    "print(\"Sample cleaned text after stopword removal:\")\n",
    "print(data[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6481971-3c36-455c-ab71-e1354a272688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 stopwords in original text:\n",
      "[('a', 2), ('you', 1), ('of', 1), ('to', 1), ('do', 1), ('it', 1), ('d', 1)]\n",
      "\n",
      "Top 10 stopwords in cleaned text:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to check stopwords removal\n",
    "def check_stopword_removal(original_text, cleaned_text):\n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenize both original and cleaned texts\n",
    "    original_tokens = word_tokenize(original_text.lower())\n",
    "    cleaned_tokens = word_tokenize(cleaned_text.lower())\n",
    "    \n",
    "    # Count stopwords in original text\n",
    "    original_stopwords = [word for word in original_tokens if word in stop_words]\n",
    "    cleaned_stopwords = [word for word in cleaned_tokens if word in stop_words]\n",
    "    \n",
    "    # Print the most common stopwords before and after removal\n",
    "    original_stopwords_freq = Counter(original_stopwords).most_common(10)\n",
    "    cleaned_stopwords_freq = Counter(cleaned_stopwords).most_common(10)\n",
    "    \n",
    "    print(\"Top 10 stopwords in original text:\")\n",
    "    print(original_stopwords_freq)\n",
    "    print(\"\\nTop 10 stopwords in cleaned text:\")\n",
    "    print(cleaned_stopwords_freq)\n",
    "\n",
    "# Test the function with a sample row\n",
    "check_stopword_removal(data['text'].iloc[0], data['cleaned_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f7c17-2efd-44ee-9a4f-8ea97b77330c",
   "metadata": {},
   "source": [
    "**Contraction Expansion**:  \n",
    "  Expanding contractions like **\"don't\"** to **\"do not\"** and **\"can't\"** to **\"cannot\"** to standardize the text. Since the apostrophes were removed previously (e.g., \"can't\" became \"cant\"), we can update the contraction dictionary to handle contractions in their modified form, without apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd12343-d1fc-41f6-8cb4-15d8dc9aae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text contractions: ['thats']\n",
      "Expanded text contractions: []\n",
      "Contractions expanded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping common contractions to their expanded forms.\n",
    "contraction_dict = {\n",
    "    \"dont\": \"do not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"im\": \"i am\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"its\": \"it is\",\n",
    "    \"were\": \"we are\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"whos\": \"who is\",\n",
    "    \"heres\": \"here is\",\n",
    "    \"theres\": \"there is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"imma\": \"i am going to\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"id\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"what'd\": \"what would\",\n",
    "    \"where'd\": \"where would\",\n",
    "    \"when'd\": \"when would\",\n",
    "    \"why'd\": \"why would\",\n",
    "    \"how'd\": \"how would\",\n",
    "    \"yall\": \"you all\",\n",
    "    \"aint\": \"is not\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"sorta\": \"sort of\",\n",
    "    \"outta\": \"out of\",\n",
    "    \"lotta\": \"lot of\",\n",
    "    \"dunno\": \"do not know\",\n",
    "    \"yknow\": \"you know\",\n",
    "    \"cmon\": \"come on\"\n",
    "}\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Replace common contractions in the given text with their expanded forms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input string containing contractions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The input text with all recognized contractions expanded.\n",
    "    \"\"\"\n",
    "    for contraction, expanded_form in contraction_dict.items():\n",
    "        text = re.sub(r'\\b' + contraction + r'\\b', expanded_form, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def check_contraction_expansion(original_text, expanded_text, contraction_dict):\n",
    "    \"\"\"\n",
    "    Check if all contractions found in the original text have been expanded in the new text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_text : str\n",
    "        The original text that may contain contractions.\n",
    "    expanded_text : str\n",
    "        The text after contraction expansion.\n",
    "    contraction_dict : dict\n",
    "        Dictionary of contractions and their expansions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if all contractions in the original text are expanded, False otherwise.\n",
    "    \"\"\"\n",
    "    orig_tokens = word_tokenize(original_text.lower())\n",
    "    exp_tokens = word_tokenize(expanded_text.lower())\n",
    "\n",
    "    orig_contractions = [w for w in orig_tokens if w in contraction_dict]\n",
    "    exp_contractions = [w for w in exp_tokens if w in contraction_dict]\n",
    "\n",
    "    print(\"Original text contractions:\", orig_contractions)\n",
    "    print(\"Expanded text contractions:\", exp_contractions)\n",
    "    return len(exp_contractions) == 0\n",
    "\n",
    "# Example usage (assuming 'data' is a DataFrame with a 'cleaned_text' column):\n",
    "data['expanded_text'] = data['cleaned_text'].apply(expand_contractions)\n",
    "\n",
    "# Test on a sample row:\n",
    "sample_original = data['cleaned_text'].iloc[0]\n",
    "sample_expanded = data['expanded_text'].iloc[0]\n",
    "\n",
    "if check_contraction_expansion(sample_original, sample_expanded, contraction_dict):\n",
    "    print(\"Contractions expanded successfully!\")\n",
    "else:\n",
    "    print(\"Some contractions were not expanded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c29699-b851-4720-b8ea-345e219a00f7",
   "metadata": {},
   "source": [
    "#### it looks like everything went well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95cda3-1bbc-45f9-ad08-70225d334e05",
   "metadata": {},
   "source": [
    "**Check for duplicate tweets**:  \n",
    "We could have missed some duplicates. now that we have the full text cleanend we can recheck again and remove those that are duplicate if they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a115e5-2217-4650-972d-54ab482d8efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates before removal: 94700\n",
      "Number of duplicates after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check the number of duplicates before removal\n",
    "initial_duplicates = data.duplicated(subset=[\"expanded_text\"]).sum()\n",
    "print(f\"Number of duplicates before removal: {initial_duplicates}\")\n",
    "\n",
    "# Step 2: Remove duplicates based on the 'expanded_text' column (update the dataset in place)\n",
    "data.drop_duplicates(subset=[\"expanded_text\"], inplace=True)\n",
    "\n",
    "# Step 3: Check for duplicates again after removal\n",
    "remaining_duplicates = data.duplicated(subset=[\"expanded_text\"]).sum()\n",
    "print(f\"Number of duplicates after removal: {remaining_duplicates}\")\n",
    "\n",
    "# Reset the index after in-place modification\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4c6b9-3475-4760-9423-7f183ae0ff46",
   "metadata": {},
   "source": [
    "**Lemmatization and Tokenization**:  \n",
    "  Breaking the text into tokens (words) and reducing them to their base form (e.g., \"running\" -> \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc72b8d-d5b6-41b9-91da-f65c55d2b778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, thats, a, bummer, you, shoulda, got, david...\n",
       "1    [is, upset, that, he, cant, update, his, faceb...\n",
       "2    [i, dived, many, time, for, the, ball, managed...\n",
       "3    [my, whole, body, feel, itchy, and, like, it, ...\n",
       "4    [no, it, not, behaving, at, all, im, mad, why,...\n",
       "Name: lemmatized_tokens, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Lemmatize each token\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "data['lemmatized_tokens'] = data['text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "data['lemmatized_tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad42872-a744-42d9-84ac-43c6819dbc60",
   "metadata": {},
   "source": [
    "**Text Vectorization**:  \n",
    "  Using techniques like **TF-IDF** or **CountVectorizer** to transform the text into numerical form for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1807d165-2f5f-46af-b5de-1f5ebfacd6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix shape: (1483537, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Join tokens into a single string per document\n",
    "data['lemmatized_text'] = data['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer with parameters to control feature space size\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # limit to 10k features\n",
    "    stop_words='english', # remove common English stopwords\n",
    "    min_df=5,            # ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.9           # ignore terms that appear in more than 90% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform the joined lemmatized text\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "print(\"TF-IDF feature matrix shape:\", tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c180f8-290a-4458-b16d-9e784bbcd841",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "This section focuses on creating new features that can be used for modeling, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb08246-e304-44db-a109-b1acef174551",
   "metadata": {},
   "source": [
    "**Temporal Features**:  \n",
    "  - Extracting features like **day of the week**, **hour of the day**, and **month** from the `date` column to capture temporal patterns in tweet activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caef6975-3c2f-41e1-ab52-e132fcbc1561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  day_of_week  hour_of_day  month\n",
      "0 2009-04-06 22:19:45-07:00            0           22      4\n",
      "1 2009-04-06 22:19:49-07:00            0           22      4\n",
      "2 2009-04-06 22:19:53-07:00            0           22      4\n",
      "3 2009-04-06 22:19:57-07:00            0           22      4\n",
      "4 2009-04-06 22:19:57-07:00            0           22      4\n"
     ]
    }
   ],
   "source": [
    "# If `date` is not yet a datetime type, convert it:\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "\n",
    "# Extract temporal features:\n",
    "data['day_of_week'] = data['date'].dt.dayofweek      # Monday=0, Sunday=6\n",
    "data['hour_of_day'] = data['date'].dt.hour\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "# For inspection:\n",
    "print(data[['date', 'day_of_week', 'hour_of_day', 'month']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a0331-eba1-490f-9059-604d2c60f3c3",
   "metadata": {},
   "source": [
    "**N-gram Features**:  \n",
    "  - Extracting **unigrams**, **bigrams**, and **trigrams** to capture patterns in word sequences that are meaningful for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15da6c8-cd20-4751-b517-0e2df92114bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix shape with n-grams: (1483537, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming `data['lemmatized_text']` contains the preprocessed text.\n",
    "# Adjusting the TfidfVectorizer to include unigrams, bigrams, and trigrams:\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,    # Limit vocabulary to top 10,000 features\n",
    "    stop_words='english',  # Remove common English stopwords\n",
    "    min_df=5,              # Ignore terms appearing in fewer than 5 documents\n",
    "    max_df=0.9,            # Ignore terms appearing in more than 90% of documents\n",
    "    ngram_range=(1, 3)     # Include unigrams, bigrams, and trigrams\n",
    ")\n",
    "\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "print(\"TF-IDF feature matrix shape with n-grams:\", tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37635f-0b6c-4ae3-984e-d49867768260",
   "metadata": {},
   "source": [
    "**Combine temporal features with TF-IDF features**:  \n",
    "  - Include the temporal features (day_of_week, hour_of_day, month) alongside the TF-IDF features, we should combine them before undersampling. This ensures that when we undersample, we remove rows consistently from both the text features and the temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7af159e0-b0d7-4029-9b09-51929e036727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract temporal features into a NumPy array\n",
    "temporal_features = data[['day_of_week', 'hour_of_day', 'month']].values\n",
    "\n",
    "# 2. Combine TF-IDF sparse matrix (X) with temporal features\n",
    "# Convert temporal_features to a sparse matrix before hstack, to keep everything sparse\n",
    "temporal_sparse = csr_matrix(temporal_features)\n",
    "\n",
    "# Combine horizontally\n",
    "X_full = hstack([tfidf_features, temporal_sparse])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea7ac0-8e68-4cf3-888e-c802ecbc17e6",
   "metadata": {},
   "source": [
    "# 4. Handling Imbalanced Data\n",
    "\n",
    "The dataset may have imbalanced sentiment classes, with a larger number of positive tweets compared to negative ones. In this section, we will:\n",
    "\n",
    "- Apply techniques like **SMOTE (Synthetic Minority Over-sampling Technique)** or **undersampling** to balance the classes and ensure that the model is not biased towards the majority class.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d13d701-fec6-43e1-810a-060f5ce5983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Undersampling:\n",
      "target\n",
      "0    750145\n",
      "1    733392\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Distribution After Undersampling:\n",
      "target\n",
      "0    733392\n",
      "1    733392\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution before undersampling\n",
    "print(\"Class Distribution Before Undersampling:\")\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "y = data['target']\n",
    "\n",
    "# Perform undersampling on the combined feature set\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_full, y)\n",
    "\n",
    "# Check class distribution after undersampling\n",
    "print(\"\\nClass Distribution After Undersampling:\")\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597891c-4358-4e19-9a0f-0a896a863ac0",
   "metadata": {},
   "source": [
    "# 5. Train/Test Split\n",
    "\n",
    "We will split the cleaned and feature-engineered dataset into **training** and **testing** sets. This will allow us to evaluate the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e0f317-e7e6-466d-8fac-ac2a4743fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved in ./modeling_data\n"
     ]
    }
   ],
   "source": [
    "# Ensure the directory exists\n",
    "os.makedirs('./modeling_data', exist_ok=True)\n",
    "\n",
    "# Perform train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled,\n",
    "    y_resampled,\n",
    "    test_size=0.2,      # 20% of data for testing\n",
    "    random_state=42,     # for reproducibility\n",
    "    stratify=y_resampled # maintain class balance in split\n",
    ")\n",
    "\n",
    "# Save the datasets for modeling\n",
    "joblib.dump(X_train, './modeling_data/ML/X_train.joblib')\n",
    "joblib.dump(X_test, './modeling_data/ML/X_test.joblib')\n",
    "joblib.dump(y_train, './modeling_data/ML/y_train.joblib')\n",
    "joblib.dump(y_test, './modeling_data/ML/y_test.joblib')\n",
    "\n",
    "print(\"Datasets saved in ./modeling_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89492b0c-4ce0-4b7c-b17a-e6d96fa38993",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "\n",
    "### Summary of Feature Engineering and Preprocessing Steps\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "- **Text Preprocessing:**  \n",
    "  - Removed stopwords to focus on meaningful tokens.\n",
    "  - Expanded contractions to ensure words are in their standard forms.\n",
    "  - Applied lemmatization to reduce words to their base forms, improving the consistency and quality of text features.\n",
    "\n",
    "- **Temporal Feature Extraction:**  \n",
    "  - Derived features such as the day of the week, hour of the day, and month from the tweet timestamps, capturing patterns that may influence sentiment.\n",
    "\n",
    "- **N-gram Creation:**  \n",
    "  - Extracted unigrams, bigrams, and trigrams, allowing the model to leverage context from sequences of words rather than single tokens alone.\n",
    "\n",
    "- **Handling Imbalanced Data:**  \n",
    "  - Employed undersampling techniques to balance the classes, ensuring the model is trained on a representative dataset.\n",
    "\n",
    "- **Data Integration and Splitting:**  \n",
    "  - Combined text-based TF-IDF features with temporal features.\n",
    "  - Split the resulting dataset into training and testing subsets for unbiased model evaluation.\n",
    "  - Saved the processed datasets for direct use in the next phase, avoiding the need for repeated preprocessing.\n",
    "\n",
    "### Next Steps and Preparation for Modeling\n",
    "\n",
    "With our dataset now fully preprocessed and enriched with a variety of engineered features, we are ready to proceed to the modeling stage. In the following notebook(s), we will:\n",
    "\n",
    "- Load the prepared data.\n",
    "- Experiment with various machine learning models.\n",
    "- Evaluate their performance using appropriate metrics.\n",
    "- Optimize the model hyperparameters to achieve the best possible results.\n",
    "\n",
    "This sets the stage for building, training, and refining robust sentiment analysis models that leverage both textual and temporal signals.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
